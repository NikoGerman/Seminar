---
title: "Estimating the Probability of Rare Events: Portfolio Risk Analysis"
author: "Nikolai German"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
```

# Introduction

Estimating probabilities of rare events presents a fundamental challenge for standard Monte Carlo methods. When events of interest occur infrequently, the resulting estimators exhibit high variance and poor convergence properties. This notebook demonstrates this challenge and the effectiveness of importance sampling through a realistic portfolio risk model.

## The Rare Event Problem

For a rare event with probability $P$, the Monte Carlo estimator $\hat{P} = \frac{1}{N}\sum_{i=1}^N \mathbf{1}(X_i \in A)$ has variance approximately $\text{Var}(\hat{P}) \approx \frac{P}{N}$ for small $P$.

The **relative error** becomes:
$$\frac{\text{SE}(\hat{P})}{\hat{P}} \approx \frac{1}{\sqrt{NP}}$$

This shows that achieving good relative accuracy for rare events requires enormous sample sizes.

## Portfolio Risk Model

Consider a portfolio initially worth 1 unit, where each stock's log-return follows a distinct normal distribution:
$Z_i \sim \mathcal{N}(\mu_i, \sigma_i^2), \quad i = 1, \ldots, 5$

The portfolio value after returns is $S = \sum_{i=1}^5 \exp(Z_i)$, where $\exp(Z_i)$ represents the final value of each asset component.

**Objective:** Estimate the tail probability $\mathbb{P}(S > 20)$, representing exceptionally high portfolio performance.

# Implementation

## Standard Monte Carlo Function

The standard approach samples log-returns directly from the original distributions:

```{r}
basic_mc <- function(N, mu, sigma, threshold) {
  params <- cbind(mu, sigma)
  V <- apply(params, MARGIN = 1, FUN = function(x) exp(rnorm(N, x[[1]], x[[2]])))
  S <- rowSums(V)
  V_max <- apply(V, MARGIN = 1, max)
  V_mean <- rowMeans(V)
  
  indicator <- (S > threshold)
  
  p <- mean(indicator)
  avg_max <- mean(V_max[indicator])
  avg_sum <- mean(S[indicator])
  mean_S <- mean(S)
  E_S <- sum(exp(mu + (sigma^2)/2))
  RE <- sd(indicator) / (p*sqrt(N))
  
  
  structure(p,
            "positive cases" = sum(indicator),
            "average maximum element" = avg_max,
            "mean sum if S > threshold" = avg_sum, 
            "mean Sum" = mean_S,
            "expected Sum" = E_S,
            "relative Error" = ifelse(is.na(RE), 1, RE)
            )
}
```

## Importance Sampling Function

For importance sampling, we sample from alternative distributions and correct using likelihood ratios:

```{r}
importance_sampling_mc <- function(N, mu, sigma, mu_tilde, sigma_tilde, threshold) {
  # Sample from importance sampling distributions
  params_is <- cbind(mu_tilde, sigma_tilde)
  
  # Generate log-returns from importance distributions (needed for weights)
  Z_tilde <- apply(params_is, MARGIN = 1, FUN = function(x) rnorm(N, x[[1]], x[[2]]))
  
  # Calculate returns (same transformation as original)
  V <- exp(Z_tilde)
  S <- rowSums(V)
  V_max <- apply(V, MARGIN = 1, max)
  V_mean <- rowMeans(V)
  
  # Calculate importance weights for each simulation
  weights <- numeric(N)
  for(k in 1:N) {
    log_weight <- 0
    for(i in 1:length(mu)) {
      log_weight <- log_weight + 
        dnorm(Z_tilde[k,i], mu[[i]], sigma[[i]], log = TRUE) - 
        dnorm(Z_tilde[k,i], mu_tilde[[i]], sigma_tilde[[i]], log = TRUE)
    }
    weights[[k]] <- exp(log_weight)
  }
  
  indicator <- (S > threshold)
  
  # Weighted probability estimate
  p <- mean(indicator * weights)
  
  # Conditional expectations using weighted averages
  if(sum(indicator) > 0) {
    weights_cond <- weights[indicator]
    avg_max <- sum(V_max[indicator] * weights_cond) / sum(weights_cond)
    avg_sum <- sum(S[indicator] * weights_cond) / sum(weights_cond)
  } else {
    avg_max <- NA
    avg_sum <- NA
  }
  
  mean_S <- mean(S * weights)
  E_S <- sum(exp(mu + (sigma^2)/2))
  
  # Relative error for weighted estimator
  weighted_indicator <- indicator * weights
  RE <- sd(weighted_indicator) / (p * sqrt(N))
  
  structure(p,
            "positive cases" = sum(indicator),
            "average maximum element" = avg_max,
            "mean sum if S > threshold" = avg_sum, 
            "mean Sum" = mean_S,
            "expected Sum" = E_S,
            "relative Error" = ifelse(is.na(RE), 1, RE))
}
```

# Initial Analysis

## Theoretical Portfolio Properties

Let's first understand the expected behavior of our portfolio:

```{r}
mu <- c(-.1, .2, -.3, .1, 0)
sigma <- sqrt(c(.3, .3, .3, .2, .2))
threshold <- 20

# Expected individual stock values: E[exp(Z_i)] = exp(mu_i + sigma_i^2/2)
expected_individual <- exp(mu + (sigma^2)/2)
expected_portfolio <- sum(expected_individual)

cat("Expected individual stock values: ", sprintf("%.3f", expected_individual))
cat(sprintf("Expected portfolio value: %.3f", expected_portfolio))
cat(sprintf("Average performance per asset: %.3f", expected_portfolio/5))
```

This shows that the expected average performance per asset is much lower than our threshold of 4, confirming that achieving a 300% profit (4x initial value) is indeed a rare event.

## Standard Monte Carlo Example

```{r}
# Run standard Monte Carlo with large sample size
set.seed(42)
N = 1e7
result <- basic_mc(N = N, 
         mu = c(-.1, .2, -.3, .1, 0),
         sigma = sqrt(c(.3, .3, .3, .2, .2)),
         threshold = 20)

cat(sprintf("Estimated probability: %.2e", result))
cat(sprintf("Number of exceedances out of %.1e samples: %d", N, attr(result, "positive cases")))
cat(sprintf("Relative error: %.2e", attr(result, "relative Error")))
```

The extremely small probability and high relative error demonstrate the challenge of rare event estimation.

## Importance Sampling Example

```{r}
# Example with importance sampling using shifted parameters
set.seed(42)
N = 1e7
result_is <- importance_sampling_mc(N = N, 
                   mu = c(-.1, .2, -.3, .1, 0),
                   sigma = sqrt(c(.3, .3, .3, .2, .2)),
                   mu_tilde = rep(0, 5),  # centered means
                   sigma_tilde = rep(1, 5), # higher variance
                   threshold = 20)

cat(sprintf("Estimated probability: %.2e", result_is))
cat(sprintf("Number of exceedances out of %.1e samples: %.2e", N, attr(result_is, "positive cases")))
cat(sprintf("Relative error: %.2e", attr(result_is, "relative Error")))
```

Notice the dramatically increased number of exceedances and reduced relative error.

# Convergence Analysis

## Sample Size Requirements

The relative error scaling $\frac{1}{\sqrt{NP}}$ means that for our rare event with $P \approx 2.3 \times 10^{-6}$:

```{r}
true_prob <- 2.3e-6  # Approximate true probability
target_relative_errors <- c(0.10, 0.05, 0.01)  # 10%, 5%, 1%

cat("Required sample sizes for different relative error targets:\n")
for (target in target_relative_errors) {
  required_N <- ceiling(1 / (target^2 * true_prob))
  cat(sprintf("For %d%% relative error: %s samples\n", 
              target*100, 
              format(required_N, big.mark = ",")))
}
```

These astronomical sample sizes show why standard Monte Carlo is impractical for this problem.

## Performance Comparison Across Sample Sizes

```{r}
set.seed(34)
df <- tibble(N = 2^seq(10, 20)) %>%
  mutate(mc = vapply(N, function(x) {basic_mc(
                    N = x, 
                    mu = c(-.1, .2, -.3, .1, 0),
                    sigma = sqrt(c(.3, .3, .3, .2, .2)),
                    threshold = 20)}, numeric(1)),
         is = vapply(N, function(x) {importance_sampling_mc(
                    N = x, 
                    mu = c(-.1, .2, -.3, .1, 0),
                    sigma = sqrt(c(.3, .3, .3, .2, .2)),
                    mu_tilde = rep(1, 5),  # shifted means
                    sigma_tilde = rep(1, 5), # increased variance
                    threshold = 20)}, numeric(1)),
         mc.error = vapply(N, function(x) {attr(basic_mc(
                    N = x, 
                    mu = c(-.1, .2, -.3, .1, 0),
                    sigma = sqrt(c(.3, .3, .3, .2, .2)),
                    threshold = 20), "relative Error")}, numeric(1)),
         is.error = vapply(N, function(x) {attr(importance_sampling_mc(
                    N = x, 
                    mu = c(-.1, .2, -.3, .1, 0),
                    sigma = sqrt(c(.3, .3, .3, .2, .2)),
                    mu_tilde = rep(1, 5),  # shifted means
                    sigma_tilde = rep(1, 5), # increased variance
                    threshold = 20), "relative Error")}, numeric(1))
         )

print(df)
```

## Efficiency Analysis

```{r}
# Calculate variance reduction factors
variance_reduction <- df %>%
  mutate(
    variance_reduction_factor = (mc.error^2) / (is.error^2),
    sample_size_equivalent = N * variance_reduction_factor
  ) %>%
  select(N, mc.error, is.error, variance_reduction_factor, sample_size_equivalent)

cat("Variance reduction analysis:\n")
print(variance_reduction %>% 
  mutate(across(where(is.numeric), ~round(., 2))))

cat("\nMean variance reduction factor:", 
    round(mean(variance_reduction$variance_reduction_factor, na.rm = TRUE), 1), "x")
```

# Visualization

## Convergence Plots

```{r}
plot_estimates <- df %>%
  select(-mc.error, -is.error) %>%
  pivot_longer(-N, names_to = "method", values_to = "estimate") %>%
  ggplot(aes(log2(N), estimate, color = method)) +
  scale_x_continuous(labels = scales::label_math(expr = 2^.x), n.breaks = 6) +
  scale_y_continuous(labels = scales::label_scientific()) +
  labs(x = "Sample Size (N)", y = "Probability Estimate", 
       title = "Convergence of Probability Estimates",
       subtitle = "Standard Monte Carlo vs Importance Sampling",
       color = "Method") +
  geom_line(size = 1) +
  geom_point(size = 2) +
  theme_minimal() +
  theme(legend.position = "bottom")

plot_rel_errors <- df %>%
  select(-mc, -is) %>%
  rename(mc = mc.error, is = is.error) %>%
  pivot_longer(-N, names_to = "method", values_to = "error") %>%
  ggplot(aes(log2(N), error, color = method)) +
  scale_x_continuous(labels = scales::label_math(expr = 2^.x), n.breaks = 6) +
  scale_y_log10(labels = scales::label_percent()) +
  labs(x = "Sample Size (N)", y = "Relative Error (log scale)", 
       title = "Relative Error Comparison",
       subtitle = "Orders of magnitude improvement with Importance Sampling",
       color = "Method") +
  geom_line(size = 1) +
  geom_point(size = 2) +
  theme_minimal() +
  theme(legend.position = "bottom")

print(plot_estimates)
print(plot_rel_errors)

# Save plots
ggsave("../Latex/figures/ex2-estimates.png", plot_estimates, device = "png", width = 8, height = 4.5)
ggsave("../Latex/figures/ex2-rel-errors.png", plot_rel_errors, device = "png", width = 8, height = 4.5)
```

## Summary Statistics

```{r}
# Final performance summary
final_comparison <- df %>%
  filter(N == max(N)) %>%
  select(N, mc, is, mc.error, is.error) %>%
  mutate(
    improvement_factor = mc.error / is.error,
    efficiency_gain = round(improvement_factor, 1)
  )

cat("Final comparison at N =", format(max(df$N), big.mark = ","), ":\n")
cat("Standard MC estimate:", scientific(final_comparison$mc, 3), "\n")
cat("Importance Sampling estimate:", scientific(final_comparison$is, 3), "\n")
cat("Standard MC relative error:", percent(final_comparison$mc.error, 0.1), "\n")
cat("Importance Sampling relative error:", percent(final_comparison$is.error, 0.1), "\n")
cat("Improvement factor:", final_comparison$efficiency_gain, "x\n")
```

# Conclusion

## Key Findings

1. **Rare Event Challenge**: The target probability $P \approx 2.3 \times 10^{-6}$ (achieving 300% profit, i.e., S/5 > 4) makes standard Monte Carlo impractical, requiring billions of samples for reasonable accuracy.

2. **Importance Sampling Effectiveness**: By shifting the sampling distribution (using $\mu_{\text{tilde}} = 1$ and $\sigma_{\text{tilde}} = 1$), we dramatically increase the frequency of rare events while maintaining correct probability estimates through importance weights.

3. **Computational Efficiency**: Importance sampling achieves 100-1000x variance reduction, making previously intractable problems feasible.

4. **Practical Impact**: This demonstrates why importance sampling is essential for risk management (estimating extreme portfolio gains of 300% profit), reliability analysis, and any application requiring accurate tail probability estimation.

## Mathematical Insight

The success of importance sampling relies on the fundamental identity:
$$\mathbb{E}_f[\mathbf{1}(X \in A)] = \mathbb{E}_g\left[\mathbf{1}(X \in A) \frac{f(X)}{g(X)}\right]$$

By choosing $g$ to concentrate probability mass in the rare event region $A$, we dramatically reduce the variance of our estimator while maintaining the correct expectation through importance weights $\frac{f(X)}{g(X)}$.

This example illustrates a fundamental principle: when direct simulation is inefficient due to the rarity of events of interest, changing the sampling distribution while maintaining mathematical correctness through importance weights can yield orders of magnitude improvements in computational efficiency.