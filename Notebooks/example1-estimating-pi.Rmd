---
title: 'Estimating $\pi$: A Comprehensive Comparison of Monte Carlo Methods'
author: "Nikolai German"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
# install.packages("SobolSequence")
library(patchwork)
library(ggplot2)
library(dplyr)
library(tidyr)
```

# Introduction

We demonstrate different sampling strategies through the well-known problem of estimating $\pi$ by approximating the area of a unit circle inscribed in a square. This classic problem illustrates the integration-sampling duality that underlies all Monte Carlo methods.

## Mathematical Foundation

Since the unit circle has area $\pi$ and the enclosing square $[-1,1] \times [-1,1]$ has area 4, we can estimate:
$$\pi = 4 \cdot \mathbb{P}(X^2 + Y^2 \leq 1)$$
where $(X,Y)$ are sampled from the square.

From an integration perspective, we want to evaluate:
$$\pi = \int_{-1}^{1} \int_{-1}^{1} \mathbf{1}(x^2 + y^2 \leq 1) \, dx \, dy$$

This demonstrates the fundamental equivalence $\mathbb{E}[f(X)] = \int f(x)p(x)dx$ - every integration problem can be reformulated as an expectation estimation problem.

# Implementation

We compare four different approaches: uniform Monte Carlo, importance sampling, quasi-Monte Carlo, and grid-based integration.

## 1. Standard Monte Carlo

```{r}
estimate_pi_mc <- function(N) {
  checkmate::assertNumeric(N, min.len = 1, any.missing = FALSE)
  inner_pi <- function(N) {
    N <- trunc(N)
    x <- runif(N, min = -1, max = 1)
    y <- runif(N, min = -1, max = 1)
    Vol <- 4
    z <- x^2 + y^2 <= 1
    Vol * sum(z) / N
  }
  estimate <- vapply(N, inner_pi, numeric(1))
  error <- abs(pi - estimate)
  structure(estimate, "error" = error)
}
```

This approach samples $(X,Y)$ uniformly from $[-1,1]^2$ and estimates $\hat{\pi} = 4 \cdot \frac{\text{points inside circle}}{N}$. This is essentially rejection sampling with expected $O(N^{-1/2})$ convergence.

## 2. Importance Sampling

```{r}
estimate_pi_is <- function(N) {
  checkmate::assertNumeric(N, min.len = 1, any.missing = FALSE)
  inner_pi <- function(N) {
    N <- trunc(N)
    x <- rnorm(N, 0, .33)
    y <- rnorm(N, 0, .33)
    f.log = dnorm(x, 0, .33, log = TRUE) + dnorm(y, 0, .33, log = TRUE)
    z <- x^2 + y^2 <= 1
    z.norm <- z / exp(f.log)
    sum(z.norm) / N  
  }
  estimate <- vapply(N, inner_pi, numeric(1))
  error <- abs(pi - estimate)
  structure(estimate, "error" = error)
}
```

We sample from $\mathcal{N}(0, \sigma^2 I_2)$ with $\sigma = 1/3$ and use the estimator $\hat{\pi} = \frac{1}{N}\sum_{i=1}^N \frac{\mathbf{1}(X_i^2 + Y_i^2 \leq 1)}{f(X_i, Y_i)}$. The normal distribution concentrates probability mass near the origin where most circle points are located.

## 3. Quasi-Monte Carlo

```{r}
estimate_pi_qmc <- function(N) {
  checkmate::assertNumeric(N, min.len = 1, any.missing = FALSE)
  inner_pi <- function(N) {
    s <- 2
    m <- floor(log(N, 2))
    mat <- SobolSequence::sobolSequence.points(dimR=s, dimF2=m, count = N)

    Vol <- 4
    z <- (2 * mat[,1] - 1)^2 + (2 * mat[,2] - 1)^2 <= 1
    Vol * sum(z) / N
  }
  estimate <- vapply(N, inner_pi, numeric(1))
  error <- abs(pi - estimate)
  structure(estimate, "error" = error)
}
```

Sobol sequences are designed to minimize gaps and clusters, providing superior space-filling properties compared to random sampling. For smooth integrands, QMC often achieves better than $O(N^{-1/2})$ convergence rates.

## 4. Equidistant Grid

```{r}
estimate_pi_grid <- function(N) {
  checkmate::assertNumeric(N, min.len = 1, any.missing = FALSE)
  inner_pi <- function(N) {
    N <- trunc(N)
    n <- sqrt(N)
    grid <- expand.grid(
      x = seq(-1, 1, length.out = n),
      y = seq(-1, 1, length.out = n)
    )
    Vol <- 4
    z <- grid$x^2 + grid$y^2 <= 1
    Vol * mean(z)  
  }
  estimate <- vapply(N, inner_pi, numeric(1))
  error <- abs(pi - estimate)
  structure(estimate, "error" = error)
}
```

Traditional deterministic quadrature evaluation on an equidistant grid. This performs well in low dimensions but suffers from the curse of dimensionality.

# Visualization of Sampling Strategies

Let's look at the different sampling approaches, using 1024 samples each.

```{r}
set.seed(123)

N = 2^10

samples_mc = tibble(x = runif(N, -1, 1), y = runif(N, -1, 1))
samples_is = tibble(x = rnorm(N, 0, .33), y = rnorm(N, 0, .33))
samples_qmc = as_tibble(SobolSequence::sobolSequence.points(2, 10, 2^10)) %>%
  rename(x = V1, y = V2) %>%
  mutate(x = 2*x - 1, y = 2*y - 1)
samples_grid = expand_grid(x = seq(-1, 1, length.out = 2^5), y = seq(-1, 1, length.out = 2^5))

p_mc <- ggplot(samples_mc, aes(x, y, color = (x^2 + y^2 <= 1))) +
  geom_point() +
  guides(color = "none") +
  theme_light() +
  ggtitle("Uniform Samples")

p_is <- p_mc %+% samples_is + ggtitle("Normal Samples")

p_qmc <- p_mc %+% samples_qmc + ggtitle("Sobol Sequence")

p_grid <- p_mc %+% samples_grid + ggtitle("Equidistant Grid")

(p_samples <- (p_grid | p_mc) / (p_is | p_qmc))
```

The visualization shows the key differences: uniform sampling exhibits clustering and gaps, normal sampling concentrates points near the origin, Sobol sequences achieve systematic coverage, while the grid provides deterministic regular placement.

```{r eval=FALSE, include=FALSE}
p_samples_small <- (p_mc | p_qmc)
ggsave("../Latex/figures/ex1_small-samples.png", p_samples_small, device = "png", width = 8, height = 4)
ggsave("../Latex/figures/ex1-samples.png", p_samples, device = "png")
```

# Convergence Analysis

Finally, visualize the convergence of the error $|\hat{\pi} - \pi|$:

```{r}
set.seed(123)
df <- tibble(N = 2^seq(10, 26, by = 2)) %>%
  mutate("Uniform MC" = attr(estimate_pi_mc(N), "error"),
         "IS" = attr(estimate_pi_is(N), "error"),
         "QMC" = attr(estimate_pi_qmc(N), "error"),
         "Grid" = attr(estimate_pi_grid(N), "error"),
         "Reference" = 1/sqrt(N))

(p_estimation_error <- df %>%
  pivot_longer(-N, names_to = "method", values_to = "error") %>%
  mutate(log.N = log2(N),
         log.error = log2(error)) %>%
  ggplot(aes(log.N, log.error, color = method)) +
  geom_line() +
  scale_x_continuous(labels = scales::label_math(expr = 2^.x)) +
  scale_y_continuous(labels = scales::label_math(expr = 2^.x)) +
  annotation_logticks() +
  labs(x = "N", 
       y = "Error", 
       color = "Method",
       title = "Estimation Error vs. Number of Samples") +
  theme_light())
```

```{r include=FALSE}
(p_estimation_error_small <- df %>%
  select(-c("IS", "Grid")) %>%
  pivot_longer(-N, names_to = "method", values_to = "error") %>%
  mutate(log.N = log2(N),
         log.error = log2(error)) %>%
  ggplot(aes(log.N, log.error, color = method)) +
  geom_line() +
  scale_x_continuous(labels = scales::label_math(expr = 2^.x)) +
  scale_y_continuous(labels = scales::label_math(expr = 2^.x)) +
  annotation_logticks() +
  labs(x = "N", 
       y = "Error", 
       color = "Method",
       title = "Estimation Error vs. Number of Samples") +
  theme_light())
```

## Results and Interpretation

**Sobol sequences** demonstrate superior performance with consistently lower errors due to their space-filling properties. **Uniform Monte Carlo** exhibits the expected $O(N^{-1/2})$ convergence with random fluctuations. **Importance sampling** performs competitively but doesn't significantly outperform uniform sampling, suggesting the normal distribution with $\sigma = 1/3$ may not provide optimal variance reduction for this problem. The **grid approach** performs reasonably well, since the dimension $d=2$ is small enough to avoid severe curse of dimensionality effects.

```{r eval=FALSE, include=FALSE}
ggsave("../Latex/figures/ex1-estimation-error.png", p_estimation_error, device = "png")
ggsave("../Latex/figures/ex1_small-estimation-error.png", p_estimation_error_small, device = "png", width = 6, height = 4)

p_overview_samll <- (p_samples_small + plot_layout(axes = "collect")) / p_estimation_error_small
ggsave("../Latex/figures/ex1_small-overview.png", p_overview_samll, device = "png", width = 6, height = 4)
```

# Integration-Sampling Duality

This example perfectly illustrates the fundamental equivalence between Monte Carlo integration and sampling. From the integration perspective, we approximate $\int_{-1}^1 \int_{-1}^1 \mathbf{1}(x^2 + y^2 \leq 1) \, dx \, dy = \pi$ using the estimator $\frac{4}{N}\sum_{i=1}^N \mathbf{1}(X_i^2 + Y_i^2 \leq 1)$. From the sampling perspective, we estimate $\mathbb{E}[\mathbf{1}(X^2 + Y^2 \leq 1)]$ where $(X,Y) \sim \text{Uniform}([-1,1]^2)$.

The identity $\mathbb{E}[f(X)] = \int f(x)p(x)dx$ reveals that expectation estimation and integration are equivalentâ€”every Monte Carlo integration can be reframed as sampling from an appropriate distribution and computing sample averages.