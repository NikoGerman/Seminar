---
title: "Monte Carlo Integration"
author: "Nikolai German"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(ggplot2)
library(patchwork)
```

# Introduction

We want to compute the integral $I = \int_0^{\pi} \sin(x) \, dx$, for which we know the true value is 2.

We use substitution with $u = g(x) = \frac{x}{\pi}$ to transform the integration domain to the unit interval $[0,1]$. With $du = g'(x) \, dx = \frac{1}{\pi} \, dx$, we obtain:

\begin{align*}
  \int_0^{\pi} \sin(x) \, dx 
  &= \pi \cdot \int_0^\pi \sin(x) \frac{1}{\pi} \, dx \\
  &= \pi \cdot \int_{g(0)}^{g(\pi)} \sin(\pi \cdot \frac{x}{\pi}) \, du \\
  &= \pi \cdot \int_{0}^{1} \sin(\pi u) \, du \\
  & \approx \frac{\pi}{n} \sum_{i=1}^n \sin(\pi U_i)
\end{align*}

where $U_i \overset{i.i.d.}{\sim} \text{Unif}(0, 1)$ for all $i = 1,...,n$.

# Visualization of Monte Carlo Integration

The following code demonstrates how Monte Carlo integration approximates the integral using different sample sizes:

```{r}
sin_pi = function(x) pi*sin(pi*x)

generate_plot <- function(n) {
  tibble(U = runif(n)) %>%
  mutate(val = sin_pi(U)) %>%
  mutate(est = mean(val),
         true = 2) %>%
  ggplot(aes(U)) +
  geom_rug() +
  geom_point(aes(y = val), color = "#83b0fc") +
  stat_function(fun = sin_pi) +
  geom_area(aes(x = seq(0,1, length.out = n), y = est), color = "#83b0fc", fill = "#83b0fc", alpha = .1, lty = "dashed") +
  geom_area(aes(x = seq(0,1, length.out = n), y = true), color = "#f8766d", fill = "#f8766d", alpha = .05) +
  scale_x_continuous(limits = c(0,1)) +
  theme_light() +
  labs(x = "U", y = "Expectation", title = sprintf("n = %d samples", n))
}

set.seed(42)
plots <- lapply(c(5, 25, 75), generate_plot)

(p_mc_int1 = (plots[[1]] | plots[[2]] | plots[[3]]) + plot_layout(axes = "collect"))
```


```{r include=FALSE}
ggsave("../Latex/figures/mc-integration1.png", plot = p_mc_int1, device = "png", width = 8, height = 4.5)
```

The plots show how the Monte Carlo estimate (blue dashed line) converges to the true value (red area) as the sample size increases. Each blue point represents a function evaluation at a random point.

# Convergence Analysis

We now analyze the convergence behavior of the Monte Carlo estimator:

```{r}
mc_est <- function(n) {
  checkmate::assertCount(n, na.ok = FALSE)
  U <- runif(n)
  est <- pi * mean(sin(pi * U))
  structure(est, samples = U)
}

set.seed(42)

df = tibble(n = 2^seq(5, 20))
df$est = vapply(df$n, mc_est, numeric(1))
df$empirical = abs(df$est - 2)
df$theoretical = 1/sqrt(df$n)

(p_mc_int2 <- df %>%
  select(-est) %>%
  pivot_longer(-n, values_to = "val", names_to = "method") %>%
  ggplot(aes(log2(n), log2(val), color = method)) +
  geom_line() +
  scale_x_continuous(labels = scales::label_math(expr = 2^.x)) +
  scale_y_continuous(labels = scales::label_math(expr = 2^.x)) +
  scale_color_manual(values = c("#83b0fc", "#f8766d")) +
  labs(x = "N", 
       y = "Error", 
       title = "Estimation Error vs. Number of Samples",
       color = "") +
  theme_light())
```


```{r include=FALSE}
ggsave("../Latex/figures/mc-integration2.png", plot = p_mc_int2, device = "png", width = 8, height = 4.5)
```

The convergence plot compares the empirical error (blue) with the theoretical $O(N^{-1/2})$ rate (red). The close alignment confirms that our Monte Carlo estimator exhibits the expected convergence behavior predicted by the Central Limit Theorem.